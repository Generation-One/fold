---
id: da1d327e-99fe-c1b3-55e5-6f984b1f948b
title: Multi-provider LLM service with automatic fallback and rate limit handling
author: system
tags:
- service
- async
- http
- auth
- database
- ai-integration
file_path: src/services/llm.rs
language: rust
memory_type: codebase
created_at: 2026-02-03T08:44:38.510943900Z
updated_at: 2026-02-03T08:44:38.510943900Z
---

This file implements a robust LLM service that abstracts multiple AI provider APIs (Gemini, Anthropic Claude, OpenRouter, OpenAI) behind a unified interface with automatic fallback capabilities. It manages provider configuration loaded from a database with environment variable seeding, handles authentication via API keys or OAuth tokens, and implements retry logic with exponential backoff for resilience. The service prioritizes providers based on configuration and gracefully degrades to alternative providers when rate limits or failures occur.